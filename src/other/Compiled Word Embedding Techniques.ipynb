{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49809401",
   "metadata": {},
   "source": [
    "# Word Embedding Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85fdb47",
   "metadata": {},
   "source": [
    "Word embeddings is a technique where individual words of a domain or language are represented as real-valued vectors in a lower dimensional space. Word embeddings are considered to be one of the successful applications of unsupervised learning at present. They do not require any annotated corpora. Embeddings use a lower-dimensional space while preserving semantic relationships.\n",
    "Some popular word embedding methods to extract features from text are:\n",
    "\n",
    "1. **Bag of words** - Bag of words is a simple and popular technique for feature extraction from text. Bag of word model processes the text to find how many times each word appeared in the sentence. This is also called as vectorization.  \n",
    "\n",
    "2. **TF-IDF** - TF-IDF is a popular word embedding technique for extracting features from corpus or vocabulary. This is a statistical method to find how important a word is to a document all over other documents.   \n",
    "\n",
    "    The full form of TF is Term Frequency. In TF, we are giving some scoring for each word or token based on the frequency of that word. The frequency of a word is dependent on the length of the document. Means in large size of document a word occurs more than a small or medium size of the documents. So to overcome this problem we will divide the frequency of a word with the length of the document (total number of words) to normalize.By using this technique also, we are creating a sparse matrix with frequency of every word.\n",
    "    \n",
    "    Formula to calculate Term Frequency (TF)\n",
    "    \n",
    "    TF = no. of times term occurrences in a document / total number of words in a document  \n",
    "    \n",
    "    The full form of IDF is Inverse Document Frequency. Here also we are assigning  a score value  to a word , this scoring  \n",
    "    value explains how a word is rare across all documents. Rarer words have more IDF score.\n",
    "\n",
    "    Formula to calculate Inverse Document Frequency (IDF) :-  \n",
    "    IDF = log base e (total number of documents / number of documents which are having term )  \n",
    "    Formula to calculate complete TF-IDF value is:\n",
    "\n",
    "    TF-IDF  = TF * IDF  \n",
    "    \n",
    "    TF-IDF value will be increased based on frequency of the word in a document. Like Bag of Words in this technique also we  \n",
    "    cannot get any semantic meaning for words.This technique is mostly used for document classification and also successfully  \n",
    "    used by search engines like Google, as a ranking factor for content. \n",
    "\n",
    "\n",
    "3. **Word2vec** - Word2vec is an algorithm invented at Google for training word embeddings. word2vec relies on the distributional hypothesis. The distributional hypothesis states that words which, often have the same neighboring words tend to be semantically similar. This helps to map semantically similar words to geometrically close embedding vectors.  \n",
    "\n",
    "4. **Fastext** - FastText is an open-source, free library from Facebook AI Research(FAIR) for learning word embeddings and word classifications. This model allows creating unsupervised learning or supervised learning algorithm for obtaining vector representations for words. It also evaluates these models. FastText supports both CBOW and Skip-gram models (*Continuous Bag of Words Model (CBOW) and Skip-gram Both are architectures to learn the underlying word representations for each word by using neural networks*).\n",
    "\n",
    "5. **ELMO (Embeddings for Language models)** - Embeddings from Language Models (ELMo) is also a powerful computational model that converts words into numbers. This vital process allows machine learning models (which take in numbers, not words, as inputs) to be trained on textual data. It achieved state-of-the-art performance on many popular tasks including question-answering, sentiment analysis, and named-entity extraction. ELMo can uniquely account for a wordâ€™s context. Previous language models such as GloVe, Bag of Words, and Word2Vec simply produce an embedding based on the literal spelling of a word. They do not factor in how the word is being used.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b8682579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "\n",
    "import gensim\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2937c0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample data\n",
    "text = [\n",
    "  'There was a man',\n",
    "  'The man had a dog',\n",
    "  'The dog and the man walked',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f3c910ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings:\n",
    "    \n",
    "    #bag of words implementation function\n",
    "    \n",
    "    def bag_of_words(text):\n",
    "    \n",
    "        '''\n",
    "        Fit a Tokenizer on the text.\n",
    "        To create tokens out of the text we will use Tokenizer class from Keras Text preprocessing module.\n",
    "        we use this model to get the vector representations of the sentences as well as to get the vocabulary.\n",
    "        '''\n",
    "        model = Tokenizer()\n",
    "        model.fit_on_texts(text)\n",
    "        print(f'Key : {list(model.word_index.keys())}') #displaying the vocabulary (tokens)\n",
    "\n",
    "        #create bag of words representation \n",
    "        '''\n",
    "        we use the text to matrix method from the tokenizer class. It converts a list of texts to a Numpy matrix. \n",
    "        By mentioning mode as count we make sure that the final matrix has the counts for each token.\n",
    "        '''\n",
    "        bow = model.texts_to_matrix(text, mode='count')\n",
    "        print(f'Bag of words :\\n',bow)\n",
    "        #return (bag_of_words)\n",
    "    \n",
    "    #=================================================================================\n",
    "    \n",
    "    #TF-IDF implementation function\n",
    "    \n",
    "    def tf_idf(text):\n",
    "        # create object\n",
    "        tfidf = TfidfVectorizer()\n",
    "\n",
    "        # get tf-idf values\n",
    "        result = tfidf.fit_transform(text)\n",
    "\n",
    "        # get idf values\n",
    "        print('\\nIDF values:')\n",
    "        for ele1, ele2 in zip(tfidf.get_feature_names(), tfidf.idf_):\n",
    "            print(ele1, ':', ele2)\n",
    "\n",
    "        # get indexing\n",
    "        print('\\nWord indexes:')\n",
    "        print(tfidf.vocabulary_)\n",
    "\n",
    "        # in matrix form\n",
    "        print('\\nTF-IDF values in matrix form:')\n",
    "        print(result.toarray())\n",
    "        #return(tf_idf)\n",
    "    \n",
    "    #=================================================================================\n",
    "    \n",
    "    # word2vec implementaion Function\n",
    "    \n",
    "    def word_2_vec(text):\n",
    "        data = []\n",
    "\n",
    "        # iterate through each sentence in the file\n",
    "        for i in text:\n",
    "            temp = []\n",
    "\n",
    "            # tokenize the sentence into words\n",
    "            for j in word_tokenize(i):\n",
    "                temp.append(j.lower())\n",
    "\n",
    "            data.append(temp)\n",
    "\n",
    "        # Creating CBOW (Continuous Bag of Words) model\n",
    "        model1 = gensim.models.Word2Vec(data, min_count = 1,\n",
    "                                    vector_size = 100, window = 5)\n",
    "\n",
    "        # Creating Skip Gram model\n",
    "        model2 = gensim.models.Word2Vec(data, min_count = 1, vector_size = 100,\n",
    "                                                    window = 5, sg = 1)\n",
    "\n",
    "        \n",
    "        print(\"Word Embeddings for Word2Vec CBOW model: \\n\", model1.wv[data[0][0]],'\\n')\n",
    "        print(\"Word Embeddings for Word2Vec Skip Gram model: \\n\", model2.wv[data[0][0]])\n",
    "\n",
    "        #return(word_2_vec)\n",
    "    \n",
    "    #=================================================================================\n",
    "    \n",
    "    #FastText Implementation Function\n",
    "\n",
    "    def fastext(text):\n",
    "        data = []\n",
    "\n",
    "        # iterate through each sentence in the file\n",
    "        for i in text:\n",
    "            temp = []\n",
    "\n",
    "            # tokenize the sentence into words\n",
    "            for j in word_tokenize(i):\n",
    "                temp.append(j.lower())\n",
    "\n",
    "            data.append(temp)\n",
    "\n",
    "        # Creating CBOW (Continuous Bag of Words) model\n",
    "        model1 = gensim.models.FastText(data, min_count = 1,\n",
    "                                    vector_size = 100, window = 5)\n",
    "\n",
    "        # Creating Skip Gram model\n",
    "        model2 = gensim.models.FastText(data, min_count = 1, vector_size = 100,\n",
    "                                                    window = 5, sg = 1)\n",
    "        \n",
    "        # Print results\n",
    "        print(\"Word Embeddings for FastText CBOW model: \\n\", model1.wv[data[0][0]],'\\n')\n",
    "        print(\"Word Embeddings for FastText Skip Gram model: \\n\", model2.wv[data[0][0]])\n",
    "\n",
    "        #return(fastext)\n",
    "    \n",
    "    #=================================================================================\n",
    "    \n",
    "    #Elmo Implementation Function   \n",
    "     \n",
    "    def elmo (text):\n",
    "        # Load pre trained ELMo model\n",
    "        elmo = hub.Module(\"https://tfhub.dev/google/elmo/3\", trainable=True)\n",
    "\n",
    "        # create an instance of ELMo\n",
    "        embeddings = elmo(text,signature=\"default\",as_dict=True)[\"elmo\"]\n",
    "        init = tf.initialize_all_variables()\n",
    "        sess = tf.Session()\n",
    "        sess.run(init)\n",
    "\n",
    "        # Print word embeddings for word MAN in given three sentences\n",
    "        print('\\nElmo Embeddings:')\n",
    "        print(sess.run(embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000b00ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "3de614d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key : ['man', 'the', 'a', 'dog', 'there', 'was', 'had', 'and', 'walked']\n",
      "Bag of words :\n",
      " [[0. 1. 0. 1. 0. 1. 1. 0. 0. 0.]\n",
      " [0. 1. 1. 1. 1. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 2. 0. 1. 0. 0. 0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "Embeddings.bag_of_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c1540b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IDF values:\n",
      "and : 1.6931471805599454\n",
      "dog : 1.2876820724517808\n",
      "had : 1.6931471805599454\n",
      "man : 1.0\n",
      "the : 1.2876820724517808\n",
      "there : 1.6931471805599454\n",
      "walked : 1.6931471805599454\n",
      "was : 1.6931471805599454\n",
      "\n",
      "Word indexes:\n",
      "{'there': 5, 'was': 7, 'man': 3, 'the': 4, 'had': 2, 'dog': 1, 'and': 0, 'walked': 6}\n",
      "\n",
      "TF-IDF values in matrix form:\n",
      "[[0.         0.         0.         0.38537163 0.         0.65249088\n",
      "  0.         0.65249088]\n",
      " [0.         0.4804584  0.63174505 0.37311881 0.4804584  0.\n",
      "  0.         0.        ]\n",
      " [0.43681766 0.33221109 0.         0.25799154 0.66442217 0.\n",
      "  0.43681766 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "Embeddings.tf_idf(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b5afab98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Embeddings for Word2Vec CBOW model: \n",
      " [-9.5785474e-03  8.9431144e-03  4.1650678e-03  9.2347339e-03\n",
      "  6.6435025e-03  2.9247357e-03  9.8040197e-03 -4.4246409e-03\n",
      " -6.8033123e-03  4.2273807e-03  3.7290000e-03 -5.6646108e-03\n",
      "  9.7047593e-03 -3.5583067e-03  9.5494054e-03  8.3472492e-04\n",
      " -6.3384580e-03 -1.9771170e-03 -7.3770545e-03 -2.9795242e-03\n",
      "  1.0416961e-03  9.4826864e-03  9.3558477e-03 -6.5958784e-03\n",
      "  3.4751510e-03  2.2755694e-03 -2.4893521e-03 -9.2291739e-03\n",
      "  1.0271263e-03 -8.1657078e-03  6.3201878e-03 -5.8000805e-03\n",
      "  5.5354382e-03  9.8337224e-03 -1.6000033e-04  4.5284913e-03\n",
      " -1.8094016e-03  7.3607611e-03  3.9400961e-03 -9.0103243e-03\n",
      " -2.3985051e-03  3.6287690e-03 -9.9568366e-05 -1.2012720e-03\n",
      " -1.0554385e-03 -1.6716027e-03  6.0495140e-04  4.1650939e-03\n",
      " -4.2527914e-03 -3.8336229e-03 -5.2816868e-05  2.6935578e-04\n",
      " -1.6880751e-04 -4.7855065e-03  4.3134023e-03 -2.1719194e-03\n",
      "  2.1035385e-03  6.6652300e-04  5.9696771e-03 -6.8423818e-03\n",
      " -6.8157101e-03 -4.4762585e-03  9.4358278e-03 -1.5918827e-03\n",
      " -9.4292425e-03 -5.4504158e-04 -4.4489242e-03  6.0000778e-03\n",
      " -9.5836855e-03  2.8590010e-03 -9.2528323e-03  1.2497997e-03\n",
      "  5.9991982e-03  7.3973467e-03 -7.6214648e-03 -6.0530235e-03\n",
      " -6.8384409e-03 -7.9183411e-03 -9.4990805e-03 -2.1254970e-03\n",
      " -8.3593366e-04 -7.2562029e-03  6.7870356e-03  1.1196184e-03\n",
      "  5.8288667e-03  1.4728665e-03  7.8936579e-04 -7.3681306e-03\n",
      " -2.1766592e-03  4.3210792e-03 -5.0853156e-03  1.1307884e-03\n",
      "  2.8833628e-03 -1.5363622e-03  9.9322936e-03  8.3496328e-03\n",
      "  2.4156666e-03  7.1182442e-03  5.8914376e-03 -5.5806185e-03] \n",
      "\n",
      "Word Embeddings for Word2Vec Skip Gram model: \n",
      " [-9.5785474e-03  8.9431144e-03  4.1650678e-03  9.2347339e-03\n",
      "  6.6435025e-03  2.9247357e-03  9.8040197e-03 -4.4246409e-03\n",
      " -6.8033123e-03  4.2273807e-03  3.7290000e-03 -5.6646108e-03\n",
      "  9.7047593e-03 -3.5583067e-03  9.5494054e-03  8.3472492e-04\n",
      " -6.3384580e-03 -1.9771170e-03 -7.3770545e-03 -2.9795242e-03\n",
      "  1.0416961e-03  9.4826864e-03  9.3558477e-03 -6.5958784e-03\n",
      "  3.4751510e-03  2.2755694e-03 -2.4893521e-03 -9.2291739e-03\n",
      "  1.0271263e-03 -8.1657078e-03  6.3201878e-03 -5.8000805e-03\n",
      "  5.5354382e-03  9.8337224e-03 -1.6000033e-04  4.5284913e-03\n",
      " -1.8094016e-03  7.3607611e-03  3.9400961e-03 -9.0103243e-03\n",
      " -2.3985051e-03  3.6287690e-03 -9.9568366e-05 -1.2012720e-03\n",
      " -1.0554385e-03 -1.6716027e-03  6.0495140e-04  4.1650939e-03\n",
      " -4.2527914e-03 -3.8336229e-03 -5.2816868e-05  2.6935578e-04\n",
      " -1.6880751e-04 -4.7855065e-03  4.3134023e-03 -2.1719194e-03\n",
      "  2.1035385e-03  6.6652300e-04  5.9696771e-03 -6.8423818e-03\n",
      " -6.8157101e-03 -4.4762585e-03  9.4358278e-03 -1.5918827e-03\n",
      " -9.4292425e-03 -5.4504158e-04 -4.4489242e-03  6.0000778e-03\n",
      " -9.5836855e-03  2.8590010e-03 -9.2528323e-03  1.2497997e-03\n",
      "  5.9991982e-03  7.3973467e-03 -7.6214648e-03 -6.0530235e-03\n",
      " -6.8384409e-03 -7.9183411e-03 -9.4990805e-03 -2.1254970e-03\n",
      " -8.3593366e-04 -7.2562029e-03  6.7870356e-03  1.1196184e-03\n",
      "  5.8288667e-03  1.4728665e-03  7.8936579e-04 -7.3681306e-03\n",
      " -2.1766592e-03  4.3210792e-03 -5.0853156e-03  1.1307884e-03\n",
      "  2.8833628e-03 -1.5363622e-03  9.9322936e-03  8.3496328e-03\n",
      "  2.4156666e-03  7.1182442e-03  5.8914376e-03 -5.5806185e-03]\n"
     ]
    }
   ],
   "source": [
    "Embeddings.word_2_vec(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "29c34a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Embeddings for FastText CBOW model: \n",
      " [ 1.00380252e-03  1.39509828e-03  1.89781445e-03  6.24475360e-04\n",
      " -1.30063141e-04  3.93523893e-04  6.32923096e-04  1.93771757e-05\n",
      " -1.76075741e-03 -2.10116617e-04 -2.47781791e-05  5.73117693e-04\n",
      "  4.78895672e-04 -2.09319856e-04  1.31731026e-03 -1.37902168e-03\n",
      "  6.72607741e-04 -1.33673998e-03 -2.25682952e-03  1.56312191e-03\n",
      "  5.91260068e-05 -7.31510925e-04  1.60478265e-03 -1.52098120e-03\n",
      " -3.72569084e-05 -1.67902559e-03  2.16971341e-04 -2.63496279e-03\n",
      " -2.17510387e-03 -1.11606430e-04  1.07013562e-03 -2.60453066e-03\n",
      "  8.77746497e-04 -2.07846123e-03  9.39855352e-04  2.38607987e-03\n",
      "  2.09304551e-03 -2.41468355e-04 -1.47287245e-03 -3.44380643e-03\n",
      "  8.93070945e-04  2.11852908e-04 -2.29015644e-03  8.90909520e-04\n",
      " -3.19046434e-03 -2.50204117e-03 -5.48833050e-04  1.69254537e-03\n",
      "  1.46050891e-03 -1.21486676e-03 -2.52983300e-03 -1.47039289e-04\n",
      "  2.34688423e-03 -1.77372748e-03  1.10516092e-03 -1.54753763e-03\n",
      " -1.49488144e-04 -2.30527972e-03 -1.33809668e-03 -8.21205613e-04\n",
      "  1.85632933e-04 -2.05830787e-03  2.28871056e-03 -8.95902805e-04\n",
      "  3.54439107e-04 -6.11904659e-04 -1.18018454e-03  8.99809762e-04\n",
      " -9.59025521e-04  3.11611773e-04 -1.54506497e-03  2.39060493e-03\n",
      "  2.61404435e-03  1.18889532e-03 -5.05017597e-05  6.09137060e-04\n",
      "  4.69602470e-04  2.45247921e-03 -7.46163831e-04  1.12559902e-03\n",
      "  7.93040905e-04  8.35360901e-04 -6.86734798e-04 -1.04911427e-03\n",
      "  2.55928934e-03 -9.23081127e-04  6.27731264e-04 -2.91961641e-03\n",
      "  5.47028030e-05 -2.73653932e-05  1.81179598e-03 -1.81016594e-03\n",
      " -2.86212249e-04 -1.54734997e-03 -1.40559452e-03  2.34031910e-03\n",
      "  3.67111410e-04  1.17034404e-04  8.38407141e-04 -4.19220625e-04] \n",
      "\n",
      "Word Embeddings for FastText Skip Gram model: \n",
      " [ 1.00380252e-03  1.39509828e-03  1.89781445e-03  6.24475360e-04\n",
      " -1.30063141e-04  3.93523893e-04  6.32923096e-04  1.93771757e-05\n",
      " -1.76075741e-03 -2.10116617e-04 -2.47781791e-05  5.73117693e-04\n",
      "  4.78895672e-04 -2.09319856e-04  1.31731026e-03 -1.37902168e-03\n",
      "  6.72607741e-04 -1.33673998e-03 -2.25682952e-03  1.56312191e-03\n",
      "  5.91260068e-05 -7.31510925e-04  1.60478265e-03 -1.52098120e-03\n",
      " -3.72569084e-05 -1.67902559e-03  2.16971341e-04 -2.63496279e-03\n",
      " -2.17510387e-03 -1.11606430e-04  1.07013562e-03 -2.60453066e-03\n",
      "  8.77746497e-04 -2.07846123e-03  9.39855352e-04  2.38607987e-03\n",
      "  2.09304551e-03 -2.41468355e-04 -1.47287245e-03 -3.44380643e-03\n",
      "  8.93070945e-04  2.11852908e-04 -2.29015644e-03  8.90909520e-04\n",
      " -3.19046434e-03 -2.50204117e-03 -5.48833050e-04  1.69254537e-03\n",
      "  1.46050891e-03 -1.21486676e-03 -2.52983300e-03 -1.47039289e-04\n",
      "  2.34688423e-03 -1.77372748e-03  1.10516092e-03 -1.54753763e-03\n",
      " -1.49488144e-04 -2.30527972e-03 -1.33809668e-03 -8.21205613e-04\n",
      "  1.85632933e-04 -2.05830787e-03  2.28871056e-03 -8.95902805e-04\n",
      "  3.54439107e-04 -6.11904659e-04 -1.18018454e-03  8.99809762e-04\n",
      " -9.59025521e-04  3.11611773e-04 -1.54506497e-03  2.39060493e-03\n",
      "  2.61404435e-03  1.18889532e-03 -5.05017597e-05  6.09137060e-04\n",
      "  4.69602470e-04  2.45247921e-03 -7.46163831e-04  1.12559902e-03\n",
      "  7.93040905e-04  8.35360901e-04 -6.86734798e-04 -1.04911427e-03\n",
      "  2.55928934e-03 -9.23081127e-04  6.27731264e-04 -2.91961641e-03\n",
      "  5.47028030e-05 -2.73653932e-05  1.81179598e-03 -1.81016594e-03\n",
      " -2.86212249e-04 -1.54734997e-03 -1.40559452e-03  2.34031910e-03\n",
      "  3.67111410e-04  1.17034404e-04  8.38407141e-04 -4.19220625e-04]\n"
     ]
    }
   ],
   "source": [
    "Embeddings.fastext(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "564c281b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Elmo Embeddings:\n",
      "[[[-0.297597   -0.06557926 -0.15180793 ... -0.27876338 -0.02799578\n",
      "   -0.09434052]\n",
      "  [ 0.22895269 -0.6115743  -0.2225335  ...  0.09913652  0.80268836\n",
      "    0.38518405]\n",
      "  [ 0.19487718 -0.20308933  0.24879742 ...  0.18695995 -0.25210536\n",
      "    0.07623564]\n",
      "  [-0.6595057  -0.25561672  0.19957364 ... -0.14810869  0.0861657\n",
      "    0.24787885]\n",
      "  [-0.02840842 -0.04353214  0.04130162 ...  0.02583169 -0.01429836\n",
      "   -0.01650422]\n",
      "  [-0.02840842 -0.04353214  0.04130162 ...  0.02583169 -0.01429836\n",
      "   -0.01650422]]\n",
      "\n",
      " [[-0.00646199  0.00602151 -0.35598326 ... -0.5774933  -0.09805511\n",
      "   -0.13721548]\n",
      "  [-0.42775753 -0.22676341 -0.07036661 ... -0.59731114  0.36165738\n",
      "    0.30080837]\n",
      "  [-0.10810221 -0.13908526 -0.6891192  ...  0.24988073  0.6034625\n",
      "    0.3689208 ]\n",
      "  [ 0.05006097 -0.19613525 -0.05395966 ...  0.2639987  -0.30015522\n",
      "    0.36188126]\n",
      "  [-0.38287795 -1.0384216   0.04224534 ...  0.06314397 -0.18220167\n",
      "    0.4078887 ]\n",
      "  [-0.02840842 -0.04353214  0.04130162 ...  0.02583169 -0.01429836\n",
      "   -0.01650422]]\n",
      "\n",
      " [[-0.00646199  0.00602151 -0.35598326 ... -0.52626693  0.20143083\n",
      "    0.151816  ]\n",
      "  [ 0.11036798 -0.7700007   0.17126796 ... -0.21957652  0.14168411\n",
      "    0.68241966]\n",
      "  [ 0.06955392 -0.2268568  -0.5919382  ... -0.27386075 -0.04257732\n",
      "    0.2726494 ]\n",
      "  [ 0.06433895 -0.22260316 -0.53894633 ... -0.15678123 -0.33360788\n",
      "   -0.19052404]\n",
      "  [-0.5435414   0.21376067 -0.25999913 ... -0.3752375   0.06995143\n",
      "    0.31991208]\n",
      "  [ 0.18347378  0.22555459  0.08953139 ...  0.24130312  0.2866557\n",
      "    0.4397113 ]]]\n"
     ]
    }
   ],
   "source": [
    "Embeddings.elmo(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fceb13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e94764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d5762f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945e3476",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
